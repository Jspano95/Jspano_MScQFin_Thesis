{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TABLE OF CONTENTS\n",
    "\n",
    "* Part one\n",
    "\n",
    "* Part Two [Calculating PECDS](#PART_TWO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import potential packages needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#system packages\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "import os \n",
    "import traceback #obs? \n",
    "if not sys.warnoptions:\n",
    "    warnings.filterwarnings(\"once\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base packages:\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from scipy import stats, integrate, optimize\n",
    "import math\n",
    "import datetime\n",
    "from datetime import date, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional packages\n",
    "\n",
    "from statsmodels.tsa.base.datetools import dates_from_str\n",
    "from IPython.display import display\n",
    "pd.options.display.max_columns = 50  #allow DF.head to show all columns in notebook\n",
    "from see import see\n",
    "from tabulate import tabulate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#packages for the econometrics / models\n",
    "\n",
    "from statsmodels.tsa.vector_ar import vecm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from arch.unitroot import DFGLS, ADF, KPSS, PhillipsPerron, ZivotAndrews\n",
    "from arch.unitroot.cointegration import engle_granger, phillips_ouliaris\n",
    "import statsmodels.formula.api as smf  #VAR package contained within \n",
    "import statsmodels.tsa.api as smt\n",
    "\n",
    "#import the functionality for detecting mathematical errors (E.G. types of linear algebra issues etc.)\n",
    "from statsmodels.tools.sm_exceptions import ValueWarning\n",
    "from arch.utility.exceptions import (\n",
    "    InfeasibleTestException,\n",
    "    InvalidLengthWarning,\n",
    "    invalid_length_doc)\n",
    "warnings.filterwarnings(\"once\", category = ValueWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1) Data Overview & Data Cleaning Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1.1) Data Overview\n",
    "\n",
    "1) First data point is a .txt file (~10GB) of Bond Data, downloaded from Wharton Research/Data Services (WRDS): pertaining to TRACE bond trades for all regular bonds (senior bonds with fixed or zero coupons) issued by firms in Compustat. Observations are at the day-level\n",
    "    * contains all bond trades between 2002 and June-2017\n",
    "    * Data included is bond ID, cusip_id, exact trade date/time, traded price, quoted_yield\n",
    "    \n",
    "2) Second data point is Bond Maturities information (From TRACE) .txt file from WRDS, relating to the corresponding data on bond maturities seniority etc.\n",
    "    * contains bond ID, cusip_id, sub-product type, debt-type, issuer_name, maturity date, grade, convertible_flag, company_symbol\n",
    "    \n",
    "3) Third data point is Bond Coupon Rates,  a .txt file from (Mergent FISD), pertaining to coupon information per bond\n",
    "    * contains: issue_id, coupon_type, offering_date, principal, first_interest_date, interest_frequency, coupon, day_count_basis, last_interest_date\n",
    "    \n",
    "4) Fourth data entry is information extracted from Fred, relating to information on swap_rates from one-year through to five-year (the risk-free rate proxy in this report)- all denonted in USD; in addition to values of VIX and other interest rate components\n",
    "\n",
    "5) Fifth data entry is the trace-mergent linkfile (provides a common factor to merge bond coupon rae information with bond market prices and bond maturities)\n",
    "    * contains: bond_ID, issue_id, issue_id_fisd\n",
    "    * This file is necessary to merge together the data about the bonds as WRDS has data on coupons, maturities, prices from different sub-vendors who potentially use different codes/reference numbers. This file will enable us to merge all of the above information on bonds and eventually, with their respective Credit Default Swap (CDS) counterparts. \n",
    "    \n",
    "6) Sixth date entry is CDS spreads (From Datastream) .txt file (>20GB) of CDS trades on the same date range as the earlier bond information. Only CDS with standard contractual clauses are considered. Observations are at the day-level\n",
    "    * contains information on: series_id, gvkey, company_name, stock_ticker, source, duration, clause, currency, class_type, date, cds_spread\n",
    "    \n",
    "7) (By earlier filtered viable GVKEY sub-sample) Equivalent Equity trading data (Daily) from WRDS/Compustat (CSV ~1GB)\n",
    "    * Includes: open, high, low, close, trading_volume......\n",
    "\n",
    "8) (By earlier filtered viable GVKEY sub-sample)(Data is monthly/quarterly level) of: Firm-level ratios\n",
    "    * pertaining to: EPS....\n",
    "    \n",
    "9) Equity Sector Data & Returns (Daily-level) from WRDS/Compustat     \n",
    "\n",
    "10) (By earlier filtered viable GVKEY sub-sample) detailed breakdown of opening, high, low, closing bond-trades data\n",
    "\n",
    "\n",
    "#### SIDENOTE: Additional robustness checks will be performed on the same data but re-sampled by weekly/monthly - these DFs will be based off the final DF which includes the merging of all the above dataframes (#1 through #10) but re-sampled respectively.\n",
    "\n",
    "11) weekly_resample_df\n",
    "\n",
    "12) monthly_resample_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the try/except needs to be changed to the more recent format - with location etc. \n",
    "\n",
    "\n",
    "#pecds_1.30 version below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bonds: Trade Day(date), bond_sym_id, price \n",
    "bond_file = open('sample_bond_data_all.txt','r')\n",
    "bonds_date_price = {}\n",
    "\n",
    "for line in bond_file:  \n",
    "  line = line.rstrip(\"\\r\\n\") \n",
    "  [bond_sym_id, date, trade_time, quantity, price, _yield] = line.split(\"\\t\")\n",
    "  if bond_sym_id==\"bond_sym_id\" and date=='date':\n",
    "    continue\n",
    "  try:\n",
    "      bonds_date_price[(date, bond_sym_id)] = price\n",
    "  except ValueError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    bonds_date_price_df = pd.DataFrame(list(bonds_date_price.items()), columns=['date','date_id_price'])\n",
    "except Exception:\n",
    "    trackback.print_exc()\n",
    "     \n",
    "bond_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a datetime object for later merging this data with the relevant swap_rates / other data\n",
    "format_date = '%Y%m%d'                                                                #American style date\n",
    "datetime_obj = bonds_date_price_df['trade_date'].apply(lambda x: datetime.datetime.strptime(x, format_date))\n",
    "\n",
    "#set the new datetime as DF index\n",
    "bonds_date_price_df = bonds_date_price_df.set_index('trade_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bond_file= open('master_file.txt','r')\n",
    "bond_maturities = {}\n",
    "\n",
    "for line in bond_file:  \n",
    "  line = line.rstrip(\"\\r\\n\") \n",
    "  [bond_sym_id, cusip_id, bsym_id, sub_prdct_type, debt_type_cd, issuer_nm, scrty_ds, cpn_rt, cpn_type_cd, trd_rpt_efctv_dt, mtrty_dt, grade, ind_144a, dissem, cnvrb_fl, company_symbol] = line.split(\"\\t\")\n",
    "  if bond_sym_id==\"bond_sym_id\":\n",
    "    continue\n",
    "  try:\n",
    "      bond_maturities[(bond_sym_id)] = mtrty_dt\n",
    "  except ValueError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    bond_maturities_df = pd.DataFrame(list(bond_maturities.items()), columns=['bond_sym_id','maturity_date'])\n",
    "except Exception:\n",
    "    trackback.print_exc()\n",
    "     \n",
    "bond_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matched with the first file by bond_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coupon_file= open('coupon_info.txt','r')\n",
    "coupons_dict = {}\n",
    "\n",
    "#change to include the location in the try/except loop\n",
    "\n",
    "for line in coupon_file:  \n",
    "  line = line.rstrip(\"\\r\\n\")\n",
    "  [ISSUE_ID, MATURITY, COUPON_TYPE, OFFERING_DATE, PRINCIPAL_AMT, FIRST_INTEREST_DATE,\n",
    "   INTEREST_FREQUENCY, COUPON, DAY_COUNT_BASIS, LAST_INTEREST_DATE] = line.split(\"\\t\")\n",
    "  if ISSUE_ID==\"ISSUE_ID\":\n",
    "    continue\n",
    "  try:\n",
    "      coupons_dict[(ISSUE_ID, INTEREST_FREQUENCY, COUPON)] = ISSUE_ID, INTEREST_FREQUENCY, COUPON\n",
    "  except ValueError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    coupons_df = pd.DataFrame(list(coupons_dict.items()), columns=['ISSUE_ID','INTEREST_FREQ'])\n",
    "except RuntimeError as re:\n",
    "    print(\"runtime error\", re)\n",
    "except Exception as other:\n",
    "    print(\"something else\", other)\n",
    " \n",
    "coupon_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RFR DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fredapi import Fred\n",
    "fred = Fred(api_key='56e2cc23702c09f0c02226f2780c4de4') #censor this code eventually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SWAP RATES\n",
    "\n",
    "\n",
    "\n",
    "get from older version \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LINEAR INTERPOLATION OF THE RFRs\n",
    "\n",
    "#interpolate / line-broken for readability\n",
    "fulldata_df[['riskfree_rate_1yr', 'riskfree_rate_2yr', 'riskfree_rate_3yr', 'riskfree_rate_4yr', 'riskfree_rate_5yr']] = \n",
    "fulldata_df[['riskfree_rate_1yr', 'riskfree_rate_2yr', 'riskfree_rate_3yr', 'riskfree_rate_4yr', 'riskfree_rate_5yr']].interpolate(method='linear',limit_direction='forward')\n",
    "\n",
    "#method = linear; consecutive NaN(s) are filled forward; axis = 1, (interpolation occurs down the columns )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VIX / TED SPREAD - also from FRED\n",
    "\n",
    "#VIX series\n",
    "cboe_vix = fred.get_series('VIXCLS', '2006-07-03','2017-06-30')\n",
    "\n",
    "#TEDRATE\n",
    "ted_rate = fred.get_series('TEDRATE', '2006-07-03','2017-06-30')\n",
    "\n",
    "#concatenate the two series column-wise\n",
    "int_spreads_concat = pd.concat([cboe_vix, ted_rate],axis=1)\n",
    "\n",
    "#convert to a dataframe\n",
    "interest_spreads_df = pd.DataFrame(int_spreads_concat.values, \n",
    "                                   index=cboe_vix.index,\n",
    "                                  columns=['cboe_vix','ted_rate'])\n",
    "\n",
    "#interpolate the DF (linearly) any missing values in the: VIX or TED rates\n",
    "interest_spreads_df[['cboe_vix','ted_rate']] = interest_spreads_df[['cboe_vix','ted_rate']].interpolate(method='linear',limit_direction='forward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check both series have zero NaN(s)\n",
    "interest_spreads_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset the index of the DF so that the trade-date becomes a seperate column (to allow merging onto the main bond/cds DF)\n",
    "interest_spreads_df = interest_spreads_df.reset_index()\n",
    "\n",
    "#rename the relevant columns:\n",
    "interest_spreads_df = interest_spreads_df.rename(columns={'index':'trade_date'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check d-types (datetime64 for trade/date and float64 for values)\n",
    "print(interest_spreads_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge this DF onto the main dateframe with the firm-level data\n",
    "* set the main-DF as the principal DF, such that a left merge on trade-date will only keep the relevant dates for each firm\n",
    "* that is, E.G. if Firm(X) has data from 2010:2014, only the interest_spreads from 2010:2014 will be affixed for this firm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#E.G. of the merge: \n",
    "fulldata_daily_vix_df = fulldata_daily_df.merge(interest_spreads_df, how='left', on='trade_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trace_mergent_linkfile.txt\n",
    "merge_file= open('trace_mergent_linkfile.txt','r')\n",
    "merge_dict = {}\n",
    "\n",
    "for line in merge_file:  \n",
    "  line = line.rstrip(\"\\r\\n\")\n",
    "  [bond_sym_id, issue_id_fisd] = line.split(\"\\t\")\n",
    "  if bond_sym_id==\"bond_sym_id\":\n",
    "    continue\n",
    "  try:\n",
    "      merge_dict[bond_sym_id, issue_id_fisd] = bond_sym_id, issue_id_fisd\n",
    "  except ValueError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    mergefile_df = pd.DataFrame(list(merge_dict.items()), columns=['bond_sym_id','issue_id_fisd'])\n",
    "except (RuntimeError, TypeError, NameError):\n",
    "    print('ERROR')\n",
    "     \n",
    "merge_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CDS SPREAD\n",
    "\n",
    "cds_file = open('sample_cds_data_all.txt','r')\n",
    "cds_spread_dict = {}\n",
    "\n",
    "for line in cds_file:  \n",
    "  line = line.rstrip(\"\\r\\n\") \n",
    "  [series_id, series_name, gvkey, company_name, ticker, source, duration, clause, currency, class_type, date, cds_spread] = line.split(\"\\t\")   \n",
    "  if series_id==\"series_id\":\n",
    "    continue\n",
    "  if (float(duration)==5 and currency==\"USD\" and clause==\"XR\"):\n",
    "    cds_spread_dict[gvkey, series_id, date] = cds_spread #date removed \n",
    "    \n",
    "cds_file.close()\n",
    "\n",
    "try: \n",
    "    cds_spread_df2 = pd.DataFrame(list(cds_spread_dict.items()),columns = ['cds_info','cds_spread']) \n",
    "except RuntimeError as re:\n",
    "    print(\"runtime error\", re)\n",
    "except Exception as other:\n",
    "    print(\"something else\", other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATAFRAME (#8) - FIRM LEVEL RATIOS\n",
    "#### E.G. of ratios contained within & outlining what the Wharton abreviations actually represent: \n",
    "\n",
    "* #bm = book/market\n",
    "* #pe_inc = P/E (Diluted, Incl. EI)\n",
    "* #ps = price/sales\n",
    "* #p/cf = Price/Cash flow\t\n",
    "* #npm = net profit margin\n",
    "* #roe = return on equity \n",
    "* #Gprof = Gross profit / Total Assets\n",
    "* #Capital_ratio = Capitalization ratio\n",
    "* #fcf_ocf = Free Cash Flow/Operating Cash Flow\n",
    "* #de_ratio = Total Debt/Equity\n",
    "* #curr_ratio = current ratio \n",
    "* #rd_sale = Research and Development/Sales\n",
    "* #ptb = price to book\n",
    "* #divyield = dividend yield \n",
    "\n",
    "#### NOTE: due to the inherently different nature of the included firms, it is expected that there will be (Null) values in each ratio per firm, E.G. not all firms will pay a dividend and thus dividend_yield will be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firm_level_ratios = 'firm_level_ratios_gvkey.csv'\n",
    "firm_level_ratio_df = pd.read_csv(firm_level_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#because of the .CSV file, the dtypes won't be compatible with the existing pandas-based DF\n",
    "#principally, the date series (for merging onto the main DF) will need to be converted to datetime from string\n",
    "\n",
    "#convert the float64 column to datetime\n",
    "#in Wharton: the series for which the ratio was released as public information is referred to as 'public_date'\n",
    "firm_level_ratios_dtime = pd.to_datetime(firm_level_ratio_df['public_date'], format='%Y%m%d', errors='coerce')\n",
    "\n",
    "#rename this column to match 'trade_date' so it can be merged \n",
    "firm_level_ratios_dtime = firm_level_ratios_dtime.rename(columns={\"public_date\": \"trade_date\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overcoming a major issue: different frequencies of reporting, as ratios per firm are reported either monthly, or quarterly:\n",
    "* Need to devise a method to interpolate the values between these two points in time and transform it to daily-level values so some series can later be used in the various models, E.G. Vector Autoregressions \n",
    "* Also, need to make sure the method cuts the time periods correctly for each firm-combination, as virtually every firm-combination has a unique set of start and finish dates for its CDS/BOND data series: both for accuracy and to reduce computation strain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#also noted that we will need to merge by [GVKEY, DATE] now and thus GVKEY needs to be converted from (object) type\n",
    "firm_level_ratios_dtime['gvkey'] = firm_level_ratios_dtime['gvkey'].astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a seperate date column which is just the month:year and leave the daily-level series unaffected\n",
    "firm_level_ratios_dtime['trade_date_month'] = firm_level_ratios_dtime['trade_date']\n",
    "\n",
    "#convert to date-time, the level of the ratios data is at minimum (month:year)\n",
    "firm_level_ratios_dtime['trade_date_month'] = pd.to_datetime(firm_level_ratios_dtime['trade_date_month'], format='%Y-%m')\n",
    "\n",
    "#create a pivot-tabel of those value-series deemed relevant for the later regression models\n",
    "df_firm_ratios_mini = firm_level_ratios_dtime.pivot(index='trade_date_month', columns='gvkey',\n",
    "                                                     values=['ps','pcf','GProf','capital_ratio','de_ratio','rd_sale'])\n",
    "\n",
    "#now, create the offsets for the DF's dates: \n",
    "start_date = df_firm_ratios_mini.index.min() - pd.DateOffset(day=1)    #note: \"trade_date_month\" is now the index column\n",
    "end_date = df_firm_ratios_mini.index.max() + pd.DateOffset(day=31)\n",
    "dates = pd.date_range(start_date, end_date, freq='D')\n",
    "dates.name = 'date'\n",
    "\n",
    "#ffill will pad the values in-between the two dates to create a continuous flow E.G. January 1, 2,... 31st etc.\n",
    "pivot_df_firm_ratios_mini = df_firm_ratios_mini.reindex(dates, method='ffill') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now, need to rearrange the pivot-DF back to a regular format by stacking the DF on the GVKEY\n",
    "pivot_df_firm_ratios_mini = pivot_df_firm_ratios_mini.stack('gvkey')\n",
    "\n",
    "#sort the DF, so that each GVKEY is sorted chronologically\n",
    "pivot_df_firm_ratios_mini = pivot_df_firm_ratios_mini.sort_index()\n",
    "\n",
    "#reset the index, so that this chronologically sorted date-series is now treated as a column within the DF (for easier merging)\n",
    "pivot_df_firm_ratios_mini = pivot_df_firm_ratios_mini.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional: create a pickle of this particular set of variables, \n",
    "#if it is required to change the input-series for a different set of values\n",
    "\n",
    "pivot_df_firm_ratios_mini.to_pickle(\"./pivot_df_firm_ratios_mini.pkl\")\n",
    "#pivot_df_firm_ratios_mini = pd.read_pickle(\"./pivot_df_firm_ratios_mini.pkl\")  #would be read-pickle code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now, the final merge to re-affix the now converted/padded values of firm-level ratios on a daily level to the main DF\n",
    "#optional to rename DF to modified version, e.g. fulldata_df_ratios to confirm the ratios are correctly formatted now\n",
    "\n",
    "fulldata_df = fulldata_df.merge(pivot_df_firm_ratios_mini, how='left', on=['gvkey','trade_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATAFRAME (#9) DAILY EQUITY PRICES / TRADING INDICATORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DVRATED -- Indicated Annual Dividend Rate - Daily\n",
    "#curcdd = CURCDD -- ISO Currency Code - Daily\n",
    "#cshoc = CSHOC -- Shares Outstanding\n",
    "#cshtrd = CSHTRD -- Trading Volume - Daily\n",
    "#eps = EPS -- Current EPS\n",
    "#prccd = PRCCD -- Price - Close - Daily\n",
    "#prchd = PRCHD -- Price - High - Daily\n",
    "#prcld = PRCLD -- Price - Low - Daily\n",
    "#prcod = PRCOD -- Price - Open - Daily\n",
    "#exchg = Stock Exchange Code\t #19 = other-OTC US, 12 = Amaerican, 14 = NASDAQ, 11 = NYSE\n",
    "#secstat = Security Status Market\t I = inactive, A= active\n",
    "#costat = COSTAT -- Active/Inactive Status Marker\n",
    "\n",
    "# = GSUBIND -- GIC Sub-Industries\n",
    "#Global Industry Classification Standard (GICS). The Sub-industry is represented by all 8 digits of the total GICS code.\n",
    "\n",
    "#idbflag = IDBFLAG -- International, Domestic, Both Indicator (B = North-America & Int, D = domestic (NA), I = int)\n",
    "#spicindcd = SPCINDCD -- S&P Industry Sector Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_us_equity_prices = 'gvkey_daily_usa_equity_prices.csv'\n",
    "daily_us_equity_prices_df = pd.read_csv(daily_us_equity_prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given this data is already at the daily-level by GVKEY, the process to merge it onto the main-DF is virtually identical to the processes outlined above. Hence, it has been omitted here to avoid needless repitition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the one thing to note here is to ensure that you keep only the one trading observation for prices per day:\n",
    "#E.G. after converting the date to datetime, renaming columns, checking for Null/NaN etc.\n",
    "\n",
    "daily_us_equity_prices_df = daily_us_equity_prices_df.drop_duplicates(subset=['gvkey','trade_date'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakingdown the data merging process:\n",
    "\n",
    "* I dropped all bonds with missing or negative prices (assume this is an error in the data on the vendor's end as it is implausible to have a negatively priced bond); repeated this step with other important data points, such as: missing coupon rates or maturity dates. \n",
    "    * Out of 10M + total observations, only a few thousand were dropped in this manner so the data integrity as whole is sound. \n",
    "    * Around 10% of the bonds have no data on interest_frequency (frequency of the coupon payments, usually semi-annual for most fixed-rate bonds); this issue is elaborated on more in one of the next steps\n",
    "\n",
    "\n",
    "* Multiple approaches in merging, I believe a fairly simple method is to perform an outer-join on the BOND & CDS data frames; thus, potentially there will be some observations with bond prices and no CDS spreads, and vice-versa. However, quickly dropping the NaN from the DF will resolve this, resulting in a DF with just the overlapping firms with both CDS and BOND data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restricting the aggregate DF to only those firms with both a CDS:Bond trading between 4-6 years\n",
    "* First, calculate the time gap between date traded (From the original bond trade file) and the maturity date by bond_ID (from the 2nd data entry file on bond maturities)- to filter the DF to only those observatios beteen four and six years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the maturity date column to more workable datetime format for the later date(1) - date(2) calculation \n",
    "fulldata_df['maturity_date'] = pd.to_datetime(fulldata_df['maturity_date'], format=\"%Y%m%d\", errors='coerce')\n",
    "\n",
    "#Extract the existing dates within the index as seperate column to simplify the subsequent calculation\n",
    "fulldata_df['time_dif'] = ((fulldata_df['maturity_date'] - fulldata_df['trade_date']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter sample to keep only observations for which both a CDS-Bond combination exist with maturities between 4-6 years\n",
    "\n",
    "#pre-create the variables to hold the date limits (note, 1Y == 365.2425 days)\n",
    "fouryear_timedelta = timedelta(days=(365.2425 * 4))\n",
    "sixyear_timedelta = timedelta(days=(365.2425 * 6))\n",
    "\n",
    "#first, restrict to only maturities less than 6 years\n",
    "fulldata_df = fulldata_df[fulldata_df['time_dif'] < pd.Timedelta(sixyear_timedelta, unit='d')]\n",
    "\n",
    "#further filter to only those maturities greater than 4 years; resulting in a range of 4-6Y\n",
    "fulldata_df = fulldata_df[fulldata_df['time_dif'] > pd.Timedelta(fouryear_timedelta, unit='d')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#improve the readability of the dataframe above by converting the days to years\n",
    "#fill NaN or missing values with 0 days\n",
    "\n",
    "fulldata_df['time_dif'] = (fulldata_df['time_dif'] / np.timedelta64(1, 'Y')).fillna(pd.Timedelta('0 Days'))\n",
    "\n",
    "#optional but recommended: round the large float value for a more compact, readable DF\n",
    "fulldata_df['time_dif'] = np.round(fulldata_df['time_dif'], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Data Cleaning & Preparation\n",
    "\n",
    "* Checking for NaN / Missing / Duplicated Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after merging all of the previous data into one aggregate frame\n",
    "#note: pickles of the intermediate data sets (E.G. just equities / bonds / cds, without ratios) exist for faster computation\n",
    "#although, pandas can handle a 1.34m row dataframe without much effort so this is largely unnecessary\n",
    "\n",
    "fulldata_df = pd.read_table('full_dataset.txt')  \n",
    "# ROWS = 13445375 \n",
    "\n",
    "fulldata_df = fulldata_df[fulldata_df['bond_sym_id'].notna()] #remove the NaN in the bond_sym_id series\n",
    "# total rows from (13445375) to 11202753\n",
    "\n",
    "fulldata_df = fulldata_df[fulldata_df['cds_spread'].notna()] #drop NaN from CDS_spread\n",
    "# total rows from 11202753 to 5495942"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting a DF of the NaN / Missing values revealed that there were a few corrupted data points with {} in place of the data\n",
    "* There were 31 instances of corrupted data points\n",
    "* Also noted several thousand NaN in the (bond) interest/coupon frequency; however, these require seperate considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find all the occurences of the corrupted data points with: {} instead of price (#NO = 31)\n",
    "corrupted_price_df = fulldata_df[fulldata_df.price.str.contains('\\{}')==True]\n",
    "\n",
    "#send the index numbers to a list\n",
    "corrupted_indexes = corrupted_price_df.index.tolist()\n",
    "\n",
    "#find the loc of these indexs so faciliate dropping them from the DF \n",
    "corrupted_indexloc = []\n",
    "for i in corrupted_indexes:\n",
    "    corrupted_indexloc.append(fulldata_df.index.get_loc(i))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop all of the rows with the corrupted {} in place of the price for the entire DF: based off their index.loc list above\n",
    "fulldata_df = fulldata_df.drop(fulldata_df.index[corrupted_indexloc], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A feature of the data is missing interest_frequency data points; these cannot simply be dropped as they potentially represent either: zero-coupon bonds, bonds which for whatever reason (perhaps near bankruptcy and undergoing debt deorganisation) have temporarily ceased paying coupons\n",
    "* These points must be accounted for in the subsequent code to calculate their par-equivalent CDS spreads & how this might affect the results as a whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect coupon_freq NaN(s)\n",
    "missing_ir_freq = fulldata_df[fulldata_df['interest_frequency'].isna()]\n",
    "missing_ir_freq\n",
    "\n",
    "#revealed 47219 instances of missing coupon frequency data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion on the data cleaning process: \n",
    "\n",
    "\n",
    "* The result is roughly 408 firms with outstanding CDS contrats, bonds which mature in 4-6 years, as well as have information available on the bonds coupons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='PART_TWO'></a>\n",
    "\n",
    "# PART TWO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Par-Equivalent Credit Default Swap (PECDS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
