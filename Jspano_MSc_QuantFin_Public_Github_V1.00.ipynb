{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import potential packages needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fredapi import Fred\n",
    "fred = Fred(api_key='56e2cc23702c09f0c02226f2780c4de4') #censor this code eventually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1) Data Overview & Data Cleaning Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1.1) Data Overview\n",
    "\n",
    "1) First data point is a .txt file (~10GB) of Bond Data, downloaded from Wharton Research/Data Services (WRDS)\n",
    "    * contains all bond trades between 2008 and 2017\n",
    "    * Data included is bond ID, cusip_id, exact trade date/time, traded price, quoted_yield\n",
    "    \n",
    "2) Second data point is a .txt file from WRDS, relating to the corresponding data on bond maturities seniority etc.\n",
    "    * contains bond ID, cusip_id, sub-product type, debt-type, issuer_name, maturity date, grade, convertible_flag, company_symbol\n",
    "    \n",
    "3) Third data point is a .txt file from WRDS, pertaining to coupon information per bond\n",
    "    * contains: issue_id, coupon_type, offering_date, principal, first_interest_date, interest_frequency, coupon, day_count_basis, last_interest_date\n",
    "    \n",
    "4) Fourth data entry is information extracted from Fred, relating to information on swap_rates from one-year through to five-year (the risk-free rate proxy in this report)- all denonted in USD; in addition to values of VIX and other interest rate components\n",
    "\n",
    "5) Fifth data entry is the trace-mergent linkfile\n",
    "    * contains: bond_ID, issue_id, issue_id_fisd\n",
    "    * This file is necessary to merge together the data about the bonds as WRDS has data on coupons, maturities, prices from different sub-vendors who potentially use different codes/reference numbers. This file will enable us to merge all of the above information on bonds and eventually, with their respective Credit Default Swap (CDS) counterparts. \n",
    "    \n",
    "6) Sixth date entry is a .txt file (>20GB) of CDS trades on the same date range as the earlier bond information, from [DATASTREAM?] \n",
    "    * contains information on: series_id, gvkey, company_name, stock_ticker, source, duration, clause, currency, class_type, date, cds_spread\n",
    "    \n",
    "7) (By earlier filtered viable GVKEY sub-sample) Equivalent Equity trading data (Daily) from WRDS/Compustat (CSV ~1GB)\n",
    "    * Includes: open, high, low, close, trading_volume......\n",
    "\n",
    "8) Equity Sector Data & Returns (Daily-level) from WRDS/Compustat \n",
    "\n",
    "9) (By earlier filtered viable GVKEY sub-sample) Firm-level ratios\n",
    "    * pertaining to: EPS....\n",
    "\n",
    "10) (By earlier filtered viable GVKEY sub-sample) detailed breakdown of opening, high, low, closing bond-trades data\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the try/except needs to be changed to the more recent format - with location etc. \n",
    "\n",
    "\n",
    "#pecds_1.30 version below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bonds: Trade Day(date), bond_sym_id, price \n",
    "bond_file = open('sample_bond_data_all.txt','r')\n",
    "bonds_date_price = {}\n",
    "\n",
    "for line in bond_file:  \n",
    "  line = line.rstrip(\"\\r\\n\") \n",
    "  [bond_sym_id, date, trade_time, quantity, price, _yield] = line.split(\"\\t\")\n",
    "  if bond_sym_id==\"bond_sym_id\" and date=='date':\n",
    "    continue\n",
    "  try:\n",
    "      bonds_date_price[(date, bond_sym_id)] = price\n",
    "  except ValueError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    bonds_date_price_df = pd.DataFrame(list(bonds_date_price.items()), columns=['date','date_id_price'])\n",
    "except Exception:\n",
    "    trackback.print_exc()\n",
    "     \n",
    "bond_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a datetime object for later merging this data with the relevant swap_rates / other data\n",
    "format_date = '%Y%m%d'                                                                #American style date\n",
    "datetime_obj = bonds_date_price_df['trade_date'].apply(lambda x: datetime.datetime.strptime(x, format_date))\n",
    "\n",
    "#set the new datetime as DF index\n",
    "bonds_date_price_df = bonds_date_price_df.set_index('trade_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bond_file= open('master_file.txt','r')\n",
    "bond_maturities = {}\n",
    "\n",
    "for line in bond_file:  \n",
    "  line = line.rstrip(\"\\r\\n\") \n",
    "  [bond_sym_id, cusip_id, bsym_id, sub_prdct_type, debt_type_cd, issuer_nm, scrty_ds, cpn_rt, cpn_type_cd, trd_rpt_efctv_dt, mtrty_dt, grade, ind_144a, dissem, cnvrb_fl, company_symbol] = line.split(\"\\t\")\n",
    "  if bond_sym_id==\"bond_sym_id\":\n",
    "    continue\n",
    "  try:\n",
    "      bond_maturities[(bond_sym_id)] = mtrty_dt\n",
    "  except ValueError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    bond_maturities_df = pd.DataFrame(list(bond_maturities.items()), columns=['bond_sym_id','maturity_date'])\n",
    "except Exception:\n",
    "    trackback.print_exc()\n",
    "     \n",
    "bond_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matched with the first file by bond_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coupon_file= open('coupon_info.txt','r')\n",
    "coupons_dict = {}\n",
    "\n",
    "for line in coupon_file:  \n",
    "  line = line.rstrip(\"\\r\\n\")\n",
    "  [ISSUE_ID, MATURITY, COUPON_TYPE, OFFERING_DATE, PRINCIPAL_AMT, FIRST_INTEREST_DATE,\n",
    "   INTEREST_FREQUENCY, COUPON, DAY_COUNT_BASIS, LAST_INTEREST_DATE] = line.split(\"\\t\")\n",
    "  if ISSUE_ID==\"ISSUE_ID\":\n",
    "    continue\n",
    "  try:\n",
    "      coupons_dict[(ISSUE_ID, INTEREST_FREQUENCY, COUPON)] = ISSUE_ID, INTEREST_FREQUENCY, COUPON\n",
    "  except ValueError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    coupons_df = pd.DataFrame(list(coupons_dict.items()), columns=['ISSUE_ID','INTEREST_FREQ'])\n",
    "except RuntimeError as re:\n",
    "    print(\"runtime error\", re)\n",
    "except Exception as other:\n",
    "    print(\"something else\", other)\n",
    " \n",
    "coupon_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RFR DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INTERPOLATION OF RFRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trace_mergent_linkfile.txt\n",
    "merge_file= open('trace_mergent_linkfile.txt','r')\n",
    "merge_dict = {}\n",
    "\n",
    "for line in merge_file:  \n",
    "  line = line.rstrip(\"\\r\\n\")\n",
    "  [bond_sym_id, issue_id_fisd] = line.split(\"\\t\")\n",
    "  if bond_sym_id==\"bond_sym_id\":\n",
    "    continue\n",
    "  try:\n",
    "      merge_dict[bond_sym_id, issue_id_fisd] = bond_sym_id, issue_id_fisd\n",
    "  except ValueError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    mergefile_df = pd.DataFrame(list(merge_dict.items()), columns=['bond_sym_id','issue_id_fisd'])\n",
    "except (RuntimeError, TypeError, NameError):\n",
    "    print('ERROR')\n",
    "     \n",
    "merge_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CDS SPREAD\n",
    "\n",
    "cds_file = open('sample_cds_data_all.txt','r')\n",
    "cds_spread_dict = {}\n",
    "\n",
    "for line in cds_file:  \n",
    "  line = line.rstrip(\"\\r\\n\") \n",
    "  [series_id, series_name, gvkey, company_name, ticker, source, duration, clause, currency, class_type, date, cds_spread] = line.split(\"\\t\")   \n",
    "  if series_id==\"series_id\":\n",
    "    continue\n",
    "  if (float(duration)==5 and currency==\"USD\" and clause==\"XR\"):\n",
    "    cds_spread_dict[gvkey, series_id, date] = cds_spread #date removed \n",
    "    \n",
    "cds_file.close()\n",
    "\n",
    "try: \n",
    "    cds_spread_df2 = pd.DataFrame(list(cds_spread_dict.items()),columns = ['cds_info','cds_spread']) \n",
    "except RuntimeError as re:\n",
    "    print(\"runtime error\", re)\n",
    "except Exception as other:\n",
    "    print(\"something else\", other)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
